# 分层可导航小世界（HNSW）算法通俗解释

> 原文来源：https://www.pinecone.io/learn/series/faiss/hnsw/
> 
> 本文是对 Pinecone 关于 HNSW 算法的详细指南的翻译和通俗化解释。

---

## 概述

**分层可导航小世界（Hierarchical Navigable Small World，HNSW）** 图是向量相似度搜索中性能最好的索引之一。HNSW 是一个非常流行的技术，一次又一次地实现了最先进的性能，具有超快的搜索速度和出色的召回率。

尽管 HNSW 是近似最近邻（ANN）搜索中流行且强大的算法，但理解它的工作原理并不容易。

本文旨在揭开 HNSW 的神秘面纱，用通俗易懂的方式解释这个智能算法。

---

## 一、HNSW 的基础概念

### 1.1 什么是 HNSW？

想象一下，你要在一个大城市里找一家餐厅。你有几种方法：

1. **树形方法**：像查字典一样，一层一层地缩小范围
2. **哈希方法**：像查电话簿一样，直接跳到某个区域
3. **图方法**：像看地图一样，沿着道路（连接）从一个地方走到另一个地方

HNSW 属于**图方法**，更具体地说，它是一种**邻近图**——两个点（向量）如果距离近，就用线（边）连接起来。

### 1.2 核心思想：分层结构

HNSW 的核心思想是**分层**：

- **顶层**：只有少数几个点，它们之间用"高速公路"（长距离连接）连接
- **中间层**：有更多的点，用"主干道"（中等距离连接）连接
- **底层**：包含所有点，用"小路"（短距离连接）连接

就像城市交通系统：
- 高速公路（顶层）让你快速跨越整个城市
- 主干道（中间层）让你在区域间快速移动
- 小路（底层）让你精确到达目的地

---

## 二、HNSW 的理论基础

### 2.1 概率跳表（Probability Skip List）

**通俗解释**：想象一个多层的地铁系统

概率跳表是 1990 年由 William Pugh 提出的。它允许像排序数组一样快速搜索，同时使用链表结构方便（且快速）地插入新元素。

**工作原理**：

```
顶层（L3）：  A ────────────────> E
             │                     │
中间层（L2）： A ──────> C ──────> E
             │         │         │
底层（L1）：  A -> B -> C -> D -> E
```

- **顶层**：连接跳过了很多中间节点（比如 A 直接到 E）
- **中间层**：跳过一些节点（比如 A 到 C，C 到 E）
- **底层**：连接所有相邻节点（A->B->C->D->E）

**搜索过程**：
1. 从顶层开始，沿着最长的"跳"移动
2. 如果当前节点的"键"大于我们要找的键，说明我们跳过了目标
3. 下降到下一层，从上一个节点继续搜索
4. 重复这个过程，直到找到目标

**HNSW 的继承**：
- HNSW 继承了这种分层格式
- 顶层有长边（快速搜索）
- 底层有短边（精确搜索）

### 2.2 可导航小世界图（Navigable Small World Graphs）

**通俗解释**：六度分隔理论

"可导航小世界"（NSW）图的概念来自 2011-2014 年的几篇论文。核心思想是：如果我们构建一个既有长距离连接又有短距离连接的邻近图，搜索时间可以降低到对数复杂度。

**工作原理**：

```
图结构：
    A ──── B
    │      │
    C ──── D
    │      │
    E ──── F
```

- 每个点（顶点）连接到几个其他点
- 这些连接的点叫做"朋友"
- 每个点维护一个"朋友列表"

**搜索过程**：

1. **从入口点开始**：从一个预定义的入口点开始搜索
2. **贪婪路由**：查看当前点的所有"朋友"，找到距离查询向量最近的那个
3. **移动到最近的点**：移动到那个最近的点
4. **重复**：重复步骤 2-3，直到找不到更近的点
5. **停止条件**：当当前点的所有"朋友"都不比当前点更近时，停止搜索

**两个阶段**：

- **"放大"阶段（Zoom-out）**：通过低度数的点（连接少的点）
- **"缩小"阶段（Zoom-in）**：通过高度数的点（连接多的点）

**问题**：
- 如果从低度数的点开始，容易过早停止（找到局部最小值）
- 如果增加平均度数，可以提高召回率，但会增加搜索时间
- 需要平衡度数和搜索速度

**解决方案**：
- 从高度数的点开始搜索（先"缩小"）
- 这在低维数据上效果很好
- 这也是 HNSW 结构的重要因素

### 2.3 HNSW 的创建

**HNSW 是 NSW 的自然演进**，它借鉴了概率跳表的分层多层结构。

**关键改进**：添加层次结构

```
HNSW 的分层结构：

顶层（L2）：        A ────────────────> E
                    │                   │
中间层（L1）：      A ──────> C ──────> E
                    │       │         │
底层（L0）：        A -> B -> C -> D -> E
```

- **顶层**：只有最长的连接，作为入口点
- **中间层**：中等长度的连接
- **底层**：所有点，最短的连接

**搜索过程**：

1. **从顶层开始**：进入顶层，这里有最长的连接
2. **贪婪搜索**：像 NSW 一样，贪婪地移动到最近的点
3. **找到局部最小值**：在当前层找不到更近的点时
4. **下降到下一层**：移动到当前点在下一层的对应位置
5. **继续搜索**：在下一层继续搜索
6. **重复**：重复步骤 2-5，直到在底层（第 0 层）找到局部最小值

**优势**：
- 默认从高度数的点开始（顶层）
- 快速跨越长距离（顶层）
- 精确找到最近邻（底层）

---

## 三、图的构建过程

### 3.1 插入过程

**通俗解释**：像建房子一样，一层一层地往上建

在构建图的过程中，向量是逐个迭代插入的。

**关键参数**：

- **L**：层数
- **m_L**：层级乘数（level multiplier），控制向量插入到各层的概率
  - `m_L ≈ 0`：向量只插入到第 0 层
  - `m_L` 越大：向量插入到更高层的概率越大

**插入规则**：

1. **计算插入层**：根据概率函数决定向量插入到哪一层
2. **插入到该层及以下所有层**：向量被添加到它的插入层和所有更低的层
3. **连接邻居**：在新插入的点周围找到最近的邻居并建立连接

**概率函数**：

```
P(插入到第 l 层) = 1 / (m_L ^ l)
```

- 第 0 层：所有向量都会插入（概率 = 1）
- 第 1 层：1/m_L 的向量会插入
- 第 2 层：1/(m_L^2) 的向量会插入
- 以此类推...

**优化目标**：

- **最小化层间共享邻居的重叠**：减少不同层之间的重复连接
- **平衡搜索时间**：不能把所有向量都放在第 0 层（会增加搜索时间）
- **经验值**：`m_L = 1/ln(2) ≈ 1.44` 是一个好的起点

### 3.2 连接建立

**通俗解释**：新来的住户要认识邻居

当插入一个新向量时：

1. **找到候选邻居**：在当前层找到距离最近的几个点
2. **选择最终邻居**：从候选邻居中选择 M 个点作为连接
3. **建立双向连接**：新点和选中的邻居之间建立连接
4. **更新邻居的连接**：如果新点比邻居的某些现有连接更近，可能会替换这些连接

**参数 M**：

- **M**：每个点最多连接的邻居数量
- M 越大：图更密集，搜索更准确，但内存占用更大
- M 越小：图更稀疏，搜索更快，但可能错过最近邻

---

## 四、HNSW 的实现

### 4.1 层级分配

**Python 实现示例**：

```python
import numpy as np

def random_level(assign_probas, rng):
    """随机决定向量插入到哪一层"""
    # 生成随机数
    r = rng.random()
    
    # 检查每一层
    for level, proba in enumerate(assign_probas):
        if r < proba:
            return level
    
    # 如果没有层满足条件，返回最高层
    return len(assign_probas) - 1
```

**层级分布**：

假设有 100 万个向量，`m_L = 1/ln(2)`：

```
第 0 层：968,821 个向量（约 97%）
第 1 层：30,170 个向量（约 3%）
第 2 层：985 个向量（约 0.1%）
第 3 层：23 个向量（约 0.002%）
第 4 层：1 个向量（约 0.0001%）
```

**重要保证**：

- 最高层至少有一个点，作为图的入口点
- 这确保了搜索总是可以从顶层开始

### 4.2 Faiss 实现

**初始化索引**：

```python
import faiss

# d: 向量维度
# M: 每个点的最大连接数
index = faiss.IndexHNSWFlat(d, M)
```

**设置参数**：

```python
# efConstruction: 构建时搜索的候选数量
index.hnsw.efConstruction = efConstruction

# 构建索引
index.add(xb)  # xb 是训练向量

# efSearch: 搜索时的候选数量
index.hnsw.efSearch = efSearch

# 搜索
index.search(xq, k)  # xq 是查询向量，k 是返回的最近邻数量
```

**参数说明**：

- **M**：每个点的最大连接数（构建时设置）
- **efConstruction**：构建索引时，为每个新点搜索的候选邻居数量
- **efSearch**：搜索时，维护的候选列表大小

---

## 五、HNSW 的性能分析

### 5.1 召回率（Recall）

**通俗解释**：找到正确答案的比例

召回率衡量搜索算法找到真正最近邻的能力。

**影响因素**：

1. **M（连接数）**：
   - M 越大 → 召回率越高
   - 但内存占用也越大

2. **efSearch（搜索候选数）**：
   - efSearch 越大 → 召回率越高
   - 但搜索时间也越长

3. **efConstruction（构建候选数）**：
   - efConstruction 越大 → 构建的图质量越好 → 召回率越高
   - 但构建时间也越长

**性能表现**：

- **高 M + 高 efSearch**：可以达到接近 100% 的召回率
- **低 M + 低 efSearch**：召回率可能只有 80% 左右
- **合理的 efConstruction**：是获得高召回率的基础

### 5.2 搜索时间

**通俗解释**：找到答案需要多长时间

**影响因素**：

1. **M**：
   - M 越大 → 搜索时间越长（需要检查更多连接）

2. **efSearch**：
   - efSearch 越大 → 搜索时间越长（需要维护更大的候选列表）

3. **efConstruction**：
   - 对搜索时间影响较小（只影响构建质量）
   - 但在大量查询时，影响会累积

**性能范围**：

- **低召回率（80%）**：搜索时间可以低至 0.1 毫秒
- **高召回率（100%）**：搜索时间可能达到 50 毫秒
- **平衡点**：通常在 1-10 毫秒之间

**单查询 vs 批量查询**：

- **单查询**：efConstruction 对搜索时间影响很小
- **批量查询（1000+）**：efConstruction 的影响会累积，变得显著

### 5.3 内存占用

**通俗解释**：需要多少内存来存储索引

**影响因素**：

- **M**：唯一影响内存的参数
- **efConstruction 和 efSearch**：不影响内存占用

**内存占用示例**（Sift1M 数据集，128 维向量）：

```
M = 2:    约 0.5 GB
M = 16:   约 1.5 GB
M = 32:   约 2.5 GB
M = 64:   约 3.5 GB
M = 128:  约 4.5 GB
M = 512:  约 5 GB
```

**内存占用大的原因**：

1. **存储所有向量**：需要存储原始向量数据
2. **存储连接信息**：每个点需要存储 M 个邻居的 ID
3. **多层结构**：不同层的连接都需要存储

### 5.4 构建时间

**通俗解释**：建立索引需要多长时间

**影响因素**：

1. **efConstruction**：
   - efConstruction 越大 → 构建时间越长
   - 但构建的图质量越好

2. **M**：
   - M 越大 → 构建时间越长（需要建立更多连接）

3. **数据量**：
   - 数据量越大 → 构建时间越长

---

## 六、性能优化策略

### 6.1 提高召回率

**策略**：

1. **增加 M**：
   - 优点：显著提高召回率
   - 缺点：内存占用和搜索时间增加

2. **增加 efSearch**：
   - 优点：提高召回率，不影响内存
   - 缺点：搜索时间增加

3. **增加 efConstruction**：
   - 优点：提高图质量，对搜索时间影响小
   - 缺点：构建时间增加

**推荐配置**：

- **高召回率需求**：M=64, efConstruction=200, efSearch=200
- **平衡配置**：M=32, efConstruction=100, efSearch=50
- **快速搜索**：M=16, efConstruction=50, efSearch=20

### 6.2 减少内存占用

**策略**：

1. **使用产品量化（Product Quantization, PQ）**：
   - 压缩向量表示
   - 可以大幅减少内存占用（减少 10-100 倍）
   - 缺点：召回率略有下降，搜索时间略有增加

2. **降低 M**：
   - 直接减少内存占用
   - 缺点：召回率下降

**示例**：

```
原始向量（128 维 float32）：512 MB
PQ 压缩（8 bits）：64 MB（减少 8 倍）
```

### 6.3 提高搜索速度

**策略**：

1. **添加 IVF 组件**：
   - 先进行粗搜索（IVF），缩小搜索范围
   - 然后在缩小范围内进行精确搜索（HNSW）
   - 可以显著提高搜索速度

2. **降低 efSearch**：
   - 直接减少搜索时间
   - 缺点：召回率下降

3. **降低 M**：
   - 减少需要检查的连接数
   - 缺点：召回率和内存占用都受影响

**组合索引示例**：

```python
# IVF + HNSW + PQ
index = faiss.index_factory(d, "IVF1024,HNSW32,PQ16")
```

---

## 七、实际应用建议

### 7.1 参数选择指南

**根据需求选择**：

1. **高召回率优先**：
   ```
   M = 64
   efConstruction = 200
   efSearch = 200
   ```

2. **快速搜索优先**：
   ```
   M = 16
   efConstruction = 50
   efSearch = 20
   ```

3. **内存受限**：
   ```
   M = 16
   efConstruction = 100
   efSearch = 50
   + 使用 PQ 压缩
   ```

4. **平衡配置**：
   ```
   M = 32
   efConstruction = 100
   efSearch = 50
   ```

### 7.2 常见问题

**Q: 为什么我的召回率很低？**

A: 可能的原因：
- M 太小：增加 M
- efSearch 太小：增加 efSearch
- efConstruction 太小：增加 efConstruction（需要重建索引）

**Q: 为什么搜索很慢？**

A: 可能的原因：
- efSearch 太大：降低 efSearch
- M 太大：降低 M（但会影响召回率）
- 数据量太大：考虑使用 IVF 进行粗搜索

**Q: 内存占用太大怎么办？**

A: 解决方案：
- 使用 PQ 压缩向量
- 降低 M
- 考虑使用其他索引类型（如 IVF + PQ）

**Q: 构建索引太慢怎么办？**

A: 解决方案：
- 降低 efConstruction（但会影响召回率）
- 使用多线程构建
- 考虑使用 GPU 加速（如果支持）

---

## 八、总结

### 8.1 HNSW 的优势

1. **高性能**：
   - 搜索速度快（毫秒级）
   - 召回率高（可达 100%）

2. **简单易用**：
   - 参数少，易于调优
   - 实现成熟，稳定可靠

3. **灵活性**：
   - 可以与其他技术组合（IVF、PQ）
   - 适应不同的应用场景

### 8.2 HNSW 的劣势

1. **内存占用大**：
   - 需要存储所有向量和连接信息
   - 对于大规模数据，内存成本高

2. **构建时间长**：
   - 需要逐个插入向量
   - 对于大规模数据，构建时间可能很长

3. **参数敏感**：
   - 参数选择对性能影响大
   - 需要根据具体场景调优

### 8.3 适用场景

**适合使用 HNSW**：

- 中小规模数据（百万级）
- 对召回率要求高
- 对搜索速度要求高
- 内存充足

**不适合使用 HNSW**：

- 超大规模数据（亿级+）
- 内存受限
- 对构建时间要求严格

### 8.4 未来方向

1. **混合索引**：
   - 结合 IVF、PQ 等技术
   - 在性能和资源之间找到平衡

2. **硬件加速**：
   - GPU 加速构建和搜索
   - 专用硬件优化

3. **自适应参数**：
   - 根据数据特征自动调整参数
   - 减少人工调优成本

---

## 九、参考资料

### 9.1 原始论文

1. **HNSW 论文**：
   - Y. Malkov, D. Yashunin, "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs" (2016), IEEE Transactions on Pattern Analysis and Machine Intelligence

2. **NSW 相关论文**：
   - Y. Malkov et al., "Approximate Nearest Neighbor Search Small World Approach" (2011)
   - Y. Malkov et al., "Scalable Distributed Algorithm for Approximate Nearest Neighbor Search Problem in High Dimensional General Metric Spaces" (2012)
   - Y. Malkov et al., "Approximate nearest neighbor algorithm based on navigable small world graphs" (2014)

3. **跳表论文**：
   - W. Pugh, "Skip lists: a probabilistic alternative to balanced trees" (1990), Communications of the ACM

### 9.2 实现参考

- **Faiss HNSW 实现**：https://github.com/facebookresearch/faiss
- **ANN Benchmarks**：https://github.com/erikbern/ann-benchmarks

### 9.3 在线资源

- **Pinecone HNSW 指南**：https://www.pinecone.io/learn/series/faiss/hnsw/
- **Faiss 文档**：https://github.com/facebookresearch/faiss/wiki

---

## 十、图片说明

由于无法直接嵌入图片，以下是文章中关键图片的描述：

### 图 1：概率跳表结构
```
顶层：    A ────────────────> E
中间层：  A ──────> C ──────> E
底层：    A -> B -> C -> D -> E
```
*说明：展示了跳表的多层结构，顶层有最长的"跳"，底层连接所有相邻节点。*

### 图 2：NSW 图搜索过程
```
入口点 A -> 最近邻 B -> 最近邻 C -> 最近邻 D（停止）
```
*说明：展示了在 NSW 图中从入口点开始，贪婪地移动到最近邻的搜索过程。*

### 图 3：HNSW 分层结构
```
顶层（L2）：        A ────────────────> E
中间层（L1）：      A ──────> C ──────> E
底层（L0）：        A -> B -> C -> D -> E
```
*说明：展示了 HNSW 的分层结构，顶层只有长连接，底层包含所有点和短连接。*

### 图 4：HNSW 搜索过程
```
顶层：A -> E（局部最小值）
      ↓ 下降
中间层：E -> C（局部最小值）
        ↓ 下降
底层：C -> D（最终结果）
```
*说明：展示了在 HNSW 中从顶层开始，逐层下降的搜索过程。*

### 图 5：层级分布
```
第 0 层：████████████████████████████████ (97%)
第 1 层：███ (3%)
第 2 层：▌ (0.1%)
第 3 层：▎ (0.002%)
第 4 层：▏ (0.0001%)
```
*说明：展示了向量在不同层级的分布，大部分向量在第 0 层，越往上越少。*

### 图 6：性能对比图表
- **召回率 vs 参数**：展示了不同 M、efConstruction、efSearch 组合下的召回率
- **搜索时间 vs 参数**：展示了不同参数组合下的搜索时间（对数刻度）
- **内存占用 vs M**：展示了 M 值对内存占用的影响

---

## 结语

HNSW 是一个强大而优雅的算法，它将概率跳表的分层思想和可导航小世界图的邻近搜索结合起来，实现了高性能的向量相似度搜索。

虽然 HNSW 在内存占用和构建时间方面有一些限制，但通过合理的参数调优和技术组合（如 PQ 压缩、IVF 粗搜索），可以在不同场景下找到最佳平衡点。

希望本文能够帮助你更好地理解 HNSW 算法，并在实际应用中做出明智的选择！

---

*本文基于 Pinecone 的 HNSW 指南翻译和改写，原文链接：https://www.pinecone.io/learn/series/faiss/hnsw/*
